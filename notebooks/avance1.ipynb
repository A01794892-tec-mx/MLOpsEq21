{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f70c5bd-ca03-409c-b56b-6b3467c86e20",
   "metadata": {},
   "source": [
    "# Proyecto de MLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "07bdeb71-9b6c-4cca-9eea-09e567bb5fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "47887d0d-75cd-4f64-8491-06cf4567fcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPipeLine:\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__( self, data_path, target_column, test_size = 0.2 ):\n",
    "        \"\"\"\n",
    "        Initialize the MLPipeline.\n",
    "\n",
    "        Args:\n",
    "            data_path (str): Path to the CSV data file.\n",
    "            target_column (str): Name of the target column.\n",
    "            test_size (float, optional): Proportion of the dataset to include in the test split. Defaults to 0.2.\n",
    "            random_state (int, optional): Random state for reproducibility. Defaults to 42.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.target_column = target_column\n",
    "        \n",
    "        self.test_size = test_size\n",
    "        self.random_state = 42\n",
    "        \n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        self.X_test = None\n",
    "        self.y_test = None\n",
    "        \n",
    "        self.pipeline = None\n",
    "        self.best_model = None\n",
    "        self.target_encoder = None\n",
    "\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\" \"\"\"\n",
    "        df_data = pd.read_csv(self.data_path)\n",
    "\n",
    "        df_data = df_data.rename(columns = { 'Classes  ': 'Classes'})\n",
    "        df_data['Classes'] = df_data['Classes'].fillna('fire')\n",
    "        df_data['Classes'] = df_data['Classes'].apply(lambda x: x.strip())\n",
    "\n",
    "\n",
    "        X = df_data.drop(self.target_column, axis = 1)\n",
    "        y = df_data[self.target_column]\n",
    "\n",
    "        # Transform categorical target to numeric\n",
    "        self.target_encoder = LabelEncoder()\n",
    "        self.target_encoder.fit(y)\n",
    "        y = self.target_encoder.transform(y)\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size = self.test_size, random_state = self.random_state)\n",
    "\n",
    "        logging.info(f\"Data loaded and split. Training set size: {len(self.X_train)}, Test set size: {len(self.X_test)}\")\n",
    "\n",
    "    def data_cleaning(self):\n",
    "        pass\n",
    "\n",
    "    def transform_data(self):        \n",
    "        pass\n",
    "\n",
    "    def create_pipeline(self):\n",
    "        \"\"\"Create the scikit-learn pipeline with preprocessing steps and the classifier.\"\"\"\n",
    "        \n",
    "        numeric_features = self.X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "        categorical_features = self.X_train.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        numeric_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "\n",
    "        ### Implementar funciones personalizadas\n",
    "        categorical_transformer = Pipeline(steps=[\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "        ])\n",
    "\n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('num', numeric_transformer, numeric_features),\n",
    "                ('cat', categorical_transformer, categorical_features)\n",
    "            ])\n",
    "\n",
    "        self.pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', LogisticRegression())\n",
    "        ])\n",
    "        logging.info(\"Pipeline created successfully\")\n",
    "\n",
    "\n",
    "    def train_model(self, param_grid):\n",
    "        \"\"\"\n",
    "        Train the model using GridSearchCV for hyperparameter tuning.\n",
    "\n",
    "        Args:\n",
    "            param_grid: Dictionary with parameters names as keys and lists of parameter settings to try as values.\n",
    "        \"\"\"\n",
    "        grid_search = GridSearchCV(self.pipeline, param_grid, cv=5, n_jobs=-1)\n",
    "        grid_search.fit(self.X_train, self.y_train)\n",
    "        self.best_model = grid_search.best_estimator_\n",
    "        logging.info(f\"Model trained. Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        predictions = self.best_model.predict(new_data)\n",
    "        predictions = self.target_encoder.inverse_transform(predictions)\n",
    "        return predictions\n",
    "\n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "44205da9-4b98-4654-83d5-ce08283bc875",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 23:39:27,062 - INFO - Data loaded and split. Training set size: 195, Test set size: 49\n",
      "2024-10-01 23:39:27,063 - INFO - Pipeline created successfully\n",
      "C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "150 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py\", line 405, in fit\n",
      "    self._final_estimator.fit(Xt, y, **fit_params_last_step)\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Omar\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [       nan 0.9025641         nan 0.95384615        nan 0.9025641\n",
      "        nan 0.95384615        nan 0.9025641         nan 0.95384615\n",
      "        nan 0.93333333        nan 0.95384615        nan 0.93333333\n",
      "        nan 0.95384615        nan 0.93333333        nan 0.95384615\n",
      "        nan 0.94358974        nan 0.95384615        nan 0.94358974\n",
      "        nan 0.95384615        nan 0.94358974        nan 0.95384615\n",
      "        nan 0.95897436        nan 0.95384615        nan 0.95897436\n",
      "        nan 0.95384615        nan 0.95897436        nan 0.95384615\n",
      "        nan 0.95897436        nan 0.95384615        nan 0.95897436\n",
      "        nan 0.95384615        nan 0.95897436        nan 0.95384615]\n",
      "  warnings.warn(\n",
      "2024-10-01 23:39:28,232 - INFO - Model trained. Best parameters: {'classifier__C': 10, 'classifier__max_iter': 100, 'classifier__penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "ml_pipeline = MLPipeLine('data/Algerian_forest_fires_dataset_UPDATE_RegionAdd.csv', 'Classes')\n",
    "ml_pipeline.load_data()\n",
    "ml_pipeline.create_pipeline()\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__penalty': ['l1', 'l2', 'elasticnet', None],  # tipo de penalización\n",
    "    'classifier__C': [0.01, 0.1, 1, 10, 100],  # inverso de la regularización\n",
    "    'classifier__max_iter': [100, 200, 300],  # número máximo de iteraciones\n",
    "}\n",
    "\n",
    "ml_pipeline.train_model(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f14bc0cc-7b2f-418e-8830-2384cf274178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for new data: ['fire' 'fire' 'fire' 'not fire' 'fire' 'not fire' 'fire' 'not fire'\n",
      " 'not fire' 'not fire' 'not fire' 'fire' 'fire' 'not fire' 'fire' 'fire'\n",
      " 'fire' 'not fire' 'fire' 'fire' 'fire' 'not fire' 'fire' 'not fire'\n",
      " 'fire' 'not fire' 'fire' 'fire' 'not fire' 'fire' 'fire' 'not fire'\n",
      " 'fire' 'fire' 'not fire' 'not fire' 'not fire' 'fire' 'not fire'\n",
      " 'not fire' 'fire' 'fire' 'not fire' 'not fire' 'fire' 'fire' 'fire'\n",
      " 'not fire' 'fire']\n"
     ]
    }
   ],
   "source": [
    "predictions = ml_pipeline.predict(ml_pipeline.X_test)\n",
    "print(\"Predictions for new data:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736b65bb-6322-422b-b00c-deeaab28eb28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
