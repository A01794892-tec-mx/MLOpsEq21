{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vzCgzkz_lY7_"
   },
   "source": [
    "# **Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "cX3bgl05lkKU"
   },
   "outputs": [],
   "source": [
    "#Importacion de librerias necesarias\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, MinMaxScaler, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vtGJ0_KlG2j",
    "outputId": "a8972630-a6e8-41f0-9ef1-7cbf555f920e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUIPs2HXl_wE"
   },
   "source": [
    "# Loading and exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "y-FEnbKVlnUk",
    "outputId": "1ad3fa4f-b526-4f2f-8142-809337055cf8"
   },
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/MNA/MLOps/data/Algerian_forest_fires_dataset_UPDATE_RegionAdd.csv\",sep=',', header='infer')\n",
    "\n",
    "def load_data(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleanup(input_file, output_file):\n",
    "\n",
    "    data = pd.read_csv(input_file ,sep=',', header='infer')\n",
    "\n",
    "    print(data)\n",
    "    print(f'Columnas: {data.columns}')\n",
    "\n",
    "    data.rename(columns={' RH':'RH', ' Ws': 'Ws', 'Classes  ':'Classes', 'Rain ':'Rain'}, inplace=True)\n",
    "\n",
    "    #data.info()\n",
    "\n",
    "    #print(f\"Classes: {data['Classes'].unique()} \\n\")\n",
    "\n",
    "    #print(f\"Region {data['Region'].unique()} \\n\")\n",
    "\n",
    "    #Detectamos que los valores de Classes contienen espacios extras por lo cual procedemos a eliminarlos\n",
    "    #El valor nulo se va a limpiar en el siguiente paso\n",
    "    data['Classes'] = data['Classes'].str.strip()\n",
    "\n",
    "    #Imprimimos de nuevo los valores unicos:\n",
    "    #print(f\"Classes: {data['Classes'].unique()} \\n\")\n",
    "\n",
    "    valores_nulos = data.isnull().sum()\n",
    "    data_limpia = data.dropna()\n",
    "\n",
    "    #Realizamos la transformacion de variables en el dataframe a su correcto formato\n",
    "    columnas_categoricas = ['Classes', 'Region']\n",
    "    columnas_enteras = ['day', 'month', 'year']\n",
    "    columnas_continuas = ['Temperature', 'RH', 'Ws', 'Rain', 'FFMC', 'DMC', 'DC', 'ISI', 'BUI', 'FWI']\n",
    "\n",
    "    data_transformada = data_limpia.copy()\n",
    "    data_transformada[columnas_categoricas] = data_limpia[columnas_categoricas].astype('category')\n",
    "    data_transformada[columnas_enteras] = data_limpia[columnas_enteras].astype('int64')\n",
    "    data_transformada[columnas_continuas] = data_limpia[columnas_continuas].astype('float64')\n",
    "\n",
    "    data_transformada.to_csv(output_file, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aplicar técnicas de preprocesamiento como normalización, codificación de variables categóricas y reducción de dimensionalidad.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_pre_proc(input_file):\n",
    "\n",
    "    data_transformada = load_data(input_file)\n",
    "\n",
    "    columnas_continuas = ['Temperature', 'RH', 'Ws', 'Rain', 'FFMC', 'DMC', 'DC', 'ISI', 'BUI', 'FWI']\n",
    "\n",
    "    y = data_transformada['Classes']\n",
    "    data_transformada.drop(columns=['Classes'], inplace=True)\n",
    "    X = data_transformada.select_dtypes(include=['float64', 'int64', 'category','object'])\n",
    "\n",
    "    #Eliminamos las columnas que no añaden valor\n",
    "    X = X.drop(columns=['day', 'month', 'year'])\n",
    "\n",
    "    #Dividimos los datos\n",
    "    X_train_base, X_test_base, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    #para las variables continuas vamos a aplicar Normalizacion usando MinMaxScaler\n",
    "    numericas_pipeline = Pipeline( steps=[\n",
    "        ('minmax', MinMaxScaler()),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('PCA', PCA(n_components=0.95))\n",
    "    ] )\n",
    "\n",
    "\n",
    "    #para las variables cate vamos a aplicar OneHot\n",
    "    catOHE_pipeline = Pipeline( steps=[\n",
    "        ('OneHotEncoder', OneHotEncoder())\n",
    "    ] )\n",
    "\n",
    "    columnas_categoricas = ['Region']\n",
    "    ct = ColumnTransformer(transformers=[\n",
    "            ('numericas_continuas', numericas_pipeline, columnas_continuas),\n",
    "            ('categoricas', catOHE_pipeline, columnas_categoricas)\n",
    "            ])\n",
    "\n",
    "    X_train = ct.fit_transform(X_train_base)\n",
    "    X_test  = ct.transform(X_test_base)\n",
    "\n",
    "    pd.DataFrame(X_train).to_csv('../data/processed/X_train.csv', index=False)\n",
    "    pd.DataFrame(X_test).to_csv('../data/processed/X_test.csv', index=False)\n",
    "    pd.DataFrame(y_train).to_csv('../data/processed/y_train.csv', index=False)\n",
    "    pd.DataFrame(y_test).to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMedN-7qCXT9"
   },
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "rFBPiWCmCeHY"
   },
   "outputs": [],
   "source": [
    "def train_model():\n",
    "    X_train=pd.read_csv('../data/processed/X_train.csv')\n",
    "    y_train=pd.read_csv('../data/processed/y_train.csv')\n",
    "\n",
    "    modeloRL = LogisticRegression(penalty='l2',\n",
    "                                  C=100,\n",
    "                                  solver='liblinear',\n",
    "                                  max_iter=1000,\n",
    "                                  random_state=55)\n",
    "\n",
    "    modeloRL.fit(X_train,np.ravel(y_train))\n",
    "\n",
    "    with open('../models/modelLR.pkl', 'wb') as f:\n",
    "        pickle.dump(modeloRL, f)\n",
    "    return modeloRL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(modelLR):\n",
    "\n",
    "    #with open('../models/modelLR.pkl', 'rb') as f:\n",
    "    #    modelLR = pickle.load(f)\n",
    "\n",
    "    X_train =   pd.read_csv('../data/processed/X_train.csv')\n",
    "    X_test  =   pd.read_csv('../data/processed/X_test.csv')\n",
    "    y_train =   pd.read_csv('../data/processed/y_train.csv')\n",
    "    y_test  =   pd.read_csv('../data/processed/y_test.csv')\n",
    "\n",
    "    print(\">>Exactitud (Accuracy) de los conjuntos de Entrenamiento y Validación con Logistic Regresion:\")\n",
    "    y_pred_trainRL = modelLR.predict(X_train)\n",
    "    y_pred_testRL = modelLR.predict(X_test)\n",
    "    print('accuracy-train', metrics.accuracy_score(y_train, y_pred_trainRL))\n",
    "    print('accuracy-test', metrics.accuracy_score(y_test, y_pred_testRL))\n",
    "\n",
    "    print(\"\\n>>Matriz de Confusión:\")\n",
    "    print(metrics.confusion_matrix(y_test, y_pred_testRL))\n",
    "\n",
    "    print(\"\\n>>Reporte varias métricas:\")\n",
    "    print(metrics.classification_report(y_test, y_pred_testRL))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(model, X, y, num_cross_validation = 5):\n",
    "    scores = cross_val_score(model, X, y, cv=num_cross_validation)\n",
    "    print(\"Average Accuracy with CV:\", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function for running the pipeline\n",
    "def main(filepath, output_file = '../data/processed/Algerian_forest_fires_dataset_clean.csv'):\n",
    "    data_cleanup(filepath, output_file)\n",
    "    data_pre_proc(output_file)\n",
    "    model_LR = train_model()\n",
    "    evaluate_model(model_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Region  day  month  year  Temperature   RH   Ws  Rain   FFMC  \\\n",
      "0            Bejaia    1      6  2012           29   57   18    0.0  65.7   \n",
      "1            Bejaia    2      6  2012           29   61   13    1.3  64.4   \n",
      "2            Bejaia    3      6  2012           26   82   22   13.1  47.1   \n",
      "3            Bejaia    4      6  2012           25   89   13    2.5  28.6   \n",
      "4            Bejaia    5      6  2012           27   77   16    0.0  64.8   \n",
      "..              ...  ...    ...   ...          ...  ...  ...    ...   ...   \n",
      "239  Sidi-Bel Abbes   26      9  2012           30   65   14    0.0  85.4   \n",
      "240  Sidi-Bel Abbes   27      9  2012           28   87   15    4.4  41.1   \n",
      "241  Sidi-Bel Abbes   28      9  2012           27   87   29    0.5  45.9   \n",
      "242  Sidi-Bel Abbes   29      9  2012           24   54   18    0.1  79.7   \n",
      "243  Sidi-Bel Abbes   30      9  2012           24   64   15    0.2  67.3   \n",
      "\n",
      "      DMC    DC  ISI   BUI  FWI     Classes    \n",
      "0     3.4   7.6  1.3   3.4  0.5   not fire     \n",
      "1     4.1   7.6  1.0   3.9  0.4   not fire     \n",
      "2     2.5   7.1  0.3   2.7  0.1   not fire     \n",
      "3     1.3   6.9  0.0   1.7    0   not fire     \n",
      "4     3.0  14.2  1.2   3.9  0.5   not fire     \n",
      "..    ...   ...  ...   ...  ...           ...  \n",
      "239  16.0  44.5  4.5  16.9  6.5       fire     \n",
      "240   6.5     8  0.1   6.2    0   not fire     \n",
      "241   3.5   7.9  0.4   3.4  0.2   not fire     \n",
      "242   4.3  15.2  1.7   5.1  0.7   not fire     \n",
      "243   3.8  16.5  1.2   4.8  0.5  not fire      \n",
      "\n",
      "[244 rows x 15 columns]\n",
      "Columnas: Index(['Region', 'day', 'month', 'year', 'Temperature', ' RH', ' Ws', 'Rain ',\n",
      "       'FFMC', 'DMC', 'DC', 'ISI', 'BUI', 'FWI', 'Classes  '],\n",
      "      dtype='object')\n",
      ">>Exactitud (Accuracy) de los conjuntos de Entrenamiento y Validación con Logistic Regresion:\n",
      "accuracy-train 0.9742268041237113\n",
      "accuracy-test 0.9387755102040817\n",
      "\n",
      ">>Matriz de Confusión:\n",
      "[[25  2]\n",
      " [ 1 21]]\n",
      "\n",
      ">>Reporte varias métricas:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        fire       0.96      0.93      0.94        27\n",
      "    not fire       0.91      0.95      0.93        22\n",
      "\n",
      "    accuracy                           0.94        49\n",
      "   macro avg       0.94      0.94      0.94        49\n",
      "weighted avg       0.94      0.94      0.94        49\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = '../data/Algerian_forest_fires_dataset_UPDATE_RegionAdd.csv'\n",
    "\n",
    "main(filepath=file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "MLOpsEq21_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
